# Assignment1

| Field | Detail |
| --- | --- |
| **Name** | ì¡°ìˆ˜ë¯¼ (cho sumin) |
| **Student ID** | 201540321 |
| **Course** | Intelligent Information Applications |
| **Date** | September 29, 2025 |

---

# 1. Introduction

This report aims to build and evaluate machine learning models for predicting wine quality based on its chemical properties, using the 'Wine Quality Data' dataset. 

For this purpose, three distinct classification models were implemented and compared: 

1. Logistic Regression 
2. k-Nearest Neighbors (k-NN)
3. Naive Bayes

The primary objective is to compare the predictive accuracy of these models and identify the most suitable one for the given dataset.

<aside>
ğŸ’¡

## Code Summary

This project was conducted to build and compare wine quality prediction models using Python libraries such asÂ `pandas`,Â `scikit-learn`, andÂ `matplotlib`.

1. **Data Preprocessing:**
    - TheÂ `Wine_Quality_Data.csv`Â dataset was loaded usingÂ `pandas`, and the target variable,Â `quality`, was separated from the features.
    - The non-numericÂ `color`Â feature was excluded from the model training.
    - The dataset was split into a training set (80%) and a testing set (20%) using theÂ `train_test_split`Â function.
    - All features were standardized usingÂ `StandardScaler`, which was fitted only on the training data to prevent data leakage.
2. **Model Training & Evaluation:**
    - **Logistic Regression:**Â AÂ `LogisticRegression`Â model fromÂ `scikit-learn`Â was trained, and its accuracy was measured on the test set. Furthermore, the model's coefficients (`coef_`) were visualized to analyze feature importance.
    - **k-Nearest Neighbors (k-NN):**Â To find the optimal hyperparameterÂ `k`, accuracies were calculated forÂ `k`Â values ranging from 1 to 30 and visualized in a plot. The value that yielded the highest performance,Â `k=9`, was selected for the final model.
    - **Naive Bayes:**Â AÂ `GaussianNB`Â model, suitable for continuous numerical data, was trained and its prediction accuracy was evaluated.
    - The performance of each model was primarily assessed usingÂ **accuracy**, andÂ **confusion matrices**Â were visualized for an intuitive understanding of the prediction results.
</aside>

# 2. Data Preprocessing

<aside>
ğŸ’¡

ë°ì´í„° ìœ ì¶œ(Data Leakage)ì€ ëª¨ë¸ì´ ì •ë‹µ(í…ŒìŠ¤íŠ¸ ë°ì´í„°)ì„ ë¯¸ë¦¬ í›”ì³ë³´ëŠ” í–‰ìœ„.

</aside>

To ensure an objective evaluation and enhance model performance, the following data preprocessing steps were performed.

- **Data Split:**Â The dataset was divided into a training set (80%) and a testing set (20%) to fairly evaluate the generalization performance of the models. This was accomplished usingÂ `sklearn.model_selection.train_test_split`Â withÂ `random_state=42`Â for reproducibility.
- **Feature Scaling:**Â Given that the features in the dataset have different units and value ranges,Â `sklearn.preprocessing.StandardScaler`Â was used to standardize all features. This step minimizes the influence of scale differences on model training, which is particularly crucial for distance-based algorithms like k-NN. To prevent data leakage, the scaler was fitted only on the training data and then used to transform both the training and testing sets.

# 3. Modeling

### 1) Logistic Regression

A standard Logistic Regression model was applied to predict the wine quality class.

```python
def multi_class_logistic_regression(input_features):

    # 1ë‹¨ê³„: ê° í´ë˜ìŠ¤(í’ˆì§ˆ ë“±ê¸‰)ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê°ê° ê³„ì‚°í•œë‹¤.
    # (ì„ í˜• ê³„ì‚°ì€ ë™ì¼í•˜ì§€ë§Œ, ì´ì œëŠ” ë“±ê¸‰ë³„ë¡œ ì ìˆ˜ê°€ í•˜ë‚˜ì”© ë‚˜ì˜µë‹ˆë‹¤.)
    score_for_quality_5 = calculate_linear_score_for_5(input_features)
    score_for_quality_6 = calculate_linear_score_for_6(input_features)
    score_for_quality_7 = calculate_linear_score_for_7(input_features)
    # ... ëª¨ë“  ë“±ê¸‰ì— ëŒ€í•´ ë°˜ë³µ ...

    all_scores = [score_for_quality_5, score_for_quality_6, score_for_quality_7, ...]

    # 2ë‹¨ê³„: ì´ ëª¨ë“  ì ìˆ˜ë¥¼ 'ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜'ì— í†µê³¼ì‹œì¼œ ê° ë“±ê¸‰ì— ì†í•  í™•ë¥ ë“¤ë¡œ ë°”ê¾¼ë‹¤.
    # (ê²°ê³¼: ëª¨ë“  í™•ë¥ ì˜ í•©ì€ 1ì´ ë©ë‹ˆë‹¤. ì˜ˆ: [0.25, 0.65, 0.10, ...])
    probabilities = softmax(all_scores)

    # 3ë‹¨ê³„: ê³„ì‚°ëœ í™•ë¥ ë“¤ ì¤‘ ê°€ì¥ ë†’ì€ ê°’ì„ ê°€ì§„ ë“±ê¸‰ì„ ìµœì¢… ê²°ê³¼ë¡œ ì„ íƒí•œë‹¤.
    # (ì˜ˆ: 6ë“±ê¸‰ì¼ í™•ë¥ ì´ 65%ë¡œ ê°€ì¥ ë†’ìœ¼ë¯€ë¡œ '6ë“±ê¸‰'ìœ¼ë¡œ ì˜ˆì¸¡)
    prediction = find_class_with_highest_probability(probabilities)

    return prediction
```

### 2) k-nn

To find the optimal hyperparameter `k`, an experiment was conducted by varying `k` from 1 to 30 and measuring the corresponding accuracy. The results showed that the model achieved its highest performance when **`k=9`**. This value was subsequently adopted for the final k-NN model.

![image.png](Assignment1%2027b77b59e042800d92a3c7bf7ea34e46/image.png)

### 3) Naive Bayes

As all features in the dataset are continuous numerical values, theÂ `GaussianNB`Â (Gaussian Naive Bayes) model, which is well-suited for such data distributions, was used for the classification task.

# 4. Results & Analysis

### 1) Model Performance Comparison:

The prediction accuracy of each model on the test data is summarized in the table below. The k-NN model, after hyperparameter tuning, achieved the highest accuracy of approximately 57%.

| Model | Accuracy |
| --- | --- |
| Logistic Regression | 0.5361 |
| K-NN (K=9) | 0.5715 |
| Naive Bayes | 0.4653 |

### 2) Confusion Matrix Analysis:

The confusion matrices for all three models revealed a common trend: they performed relatively well in classifying wines of average quality (grades 5 and 6), which constitute the majority of the data. However, they struggled to accurately predict wines at the extreme ends of the quality spectrum, where data is sparse.

- **Logistic Regression**
    
    ![image.png](Assignment1%2027b77b59e042800d92a3c7bf7ea34e46/image%201.png)
    
- **K-NN**
    
    ![image.png](Assignment1%2027b77b59e042800d92a3c7bf7ea34e46/image%202.png)
    
- **Naive-Bayes**
    
    ![image.png](Assignment1%2027b77b59e042800d92a3c7bf7ea34e46/image%203.png)
    

### 3) Feature Importance Analysis:

By analyzing the learned coefficients (coef_) of the Logistic Regression model, key features influencing wine quality prediction were identified. The analysis indicated that features such as 'alcohol' and 'volatile acidity' were significant predictors. This analysis was limited to Logistic Regression, as k-NN and Naive Bayes models do not provide coefficients in a directly interpretable manner.

![image.png](Assignment1%2027b77b59e042800d92a3c7bf7ea34e46/image%204.png)

# 5. Conclusion

<aside>
ğŸ’¡

Best Model : k-NN model(k = 9)

</aside>

This study compared the performance of Logistic Regression, k-NN, and Naive Bayes models for predicting wine quality. TheÂ **k-NN model, with an optimizedÂ `k`Â value of 9, demonstrated the highest accuracy at 57.15%**. This result highlights the importance of proper hyperparameter tuning in improving model performance. Furthermore, the analysis of the Logistic Regression model provided valuable insights into which chemical properties are most influential in determining wine quality.

However, the overall accuracy of the models did not exceed 60%, which may be attributed to theÂ **class imbalance**Â issue present in the original dataset.

<aside>
ğŸ’¡

ë°ì´í„°ë“¤ ëŒ€ë¶€ë¶„ì´ quality = 5,6,7ì´ë‹¤.

</aside>

Future work could focus on addressing this imbalance through techniques like oversampling. Additionally, incorporating the categorical feature 'color' via one-hot encoding could potentially lead to further improvements in prediction accuracy.